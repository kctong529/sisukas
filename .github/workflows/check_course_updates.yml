name: Check Course Updates

on:
  workflow_dispatch:
  schedule:
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'
  push:
    branches:
      - main
      - dev

env:
  PYTHON_VERSION: '3.12'

jobs:
  prepare_data:
    runs-on: ubuntu-latest
    permissions:
      contents: 'read'
      id-token: 'write'
    outputs:
      has_changes: ${{ steps.check_changes.outputs.has_changes }}
      update_historical: ${{ steps.scope.outputs.update_historical }}
      gcs_bucket: ${{ steps.set_bucket.outputs.gcs_bucket }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      # Determine bucket based on branch
      - name: Select GCS bucket
        id: set_bucket
        run: |
          if [ "${GITHUB_REF##*/}" = "main" ]; then
            echo "gcs_bucket=sisukas-core" >> $GITHUB_OUTPUT
          else
            echo "gcs_bucket=sisukas-core-test" >> $GITHUB_OUTPUT
          fi

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: pip install requests

      - name: Fetch existing courses.json if available
        run: |
          curl -fSL \
          "https://storage.googleapis.com/${{ steps.set_bucket.outputs.gcs_bucket }}/courses.json" \
          -o courses.json || echo "No existing courses.json"

      - name: Fetch existing historical.json if available
        run: |
          curl -fSL \
          "https://storage.googleapis.com/${{ steps.set_bucket.outputs.gcs_bucket }}/historical.json" \
          -o historical.json || echo "No existing historical.json"
        
      - name: Fetch latest courses
        env:
          AALTO_USER_KEY: ${{ secrets.AALTO_USER_KEY }}
        run: python scripts/fetch_latest_courses.py

      - name: Transform latest fetch
        run: |
          python scripts/transform_courses.py latest_fetch.json latest.json

      - name: Check for changes (and write removed objects)
        id: check_changes
        run: |
          mkdir -p artifacts
          python scripts/report_course_changes.py --removed-out artifacts/expired_courses.json
      
      - name: Count reactivated instances (historical âˆ© latest)
        id: reactivated
        run: |
          python scripts/report_reactivated.py --historical historical.json --active latest.json
      
      - name: Decide whether to update historical
        id: scope
        run: |
          REMOVED="${{ steps.check_changes.outputs.removed_count }}"
          REACTIVATED="${{ steps.reactivated.outputs.reactivated_count }}"
          if [ "${REMOVED:-0}" != "0" ] || [ "${REACTIVATED:-0}" != "0" ]; then
            echo "update_historical=true" >> $GITHUB_OUTPUT
          else
            echo "update_historical=false" >> $GITHUB_OUTPUT
          fi

      - name: Prepare artifacts
        if: steps.check_changes.outputs.has_changes == 'true'
        run: |
          mkdir -p artifacts
          cp latest.json artifacts/courses.next.json
      
      - name: Prepare historical artifacts
        if: steps.check_changes.outputs.has_changes == 'true' && steps.scope.outputs.update_historical == 'true'
        run: |
          mkdir -p artifacts
          python scripts/merge_expired_into_historical.py \
            historical.json artifacts/expired_courses.json latest.json artifacts/historical.next.json

      - name: Validate active/historical disjointness
        if: steps.scope.outputs.update_historical == 'true'
        run: |
          python scripts/validate_disjoint_datasets.py \
            --a artifacts/courses.next.json --label-a active \
            --b artifacts/historical.next.json --label-b historical

      - name: Package artifacts
        if: steps.check_changes.outputs.has_changes == 'true'
        run: |
          gzip -c artifacts/courses.next.json > artifacts/courses.json.gz
          python scripts/generate_hash.py artifacts/courses.next.json artifacts/courses.hash.json

          if [ "${{ steps.scope.outputs.update_historical }}" = "true" ]; then
            gzip -c artifacts/historical.next.json > artifacts/historical.json.gz
            python scripts/generate_hash.py artifacts/historical.next.json artifacts/historical.hash.json
          fi

          LATEST_LOG=$(ls -1t logs/course_changes_*.json | head -n 1)
          cp "$LATEST_LOG" artifacts/

      - name: Upload artifacts (internal)
        if: steps.check_changes.outputs.has_changes == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: courses-artifacts
          path: artifacts/

  upload_to_gcs:
    needs: prepare_data
    runs-on: ubuntu-latest
    if: needs.prepare_data.outputs.has_changes == 'true'
    permissions:
      id-token: 'write'
    steps:
      - uses: actions/checkout@v4

      - id: auth
        uses: google-github-actions/auth@v3
        with:
          project_id: 'sisukas-fastapi'
          workload_identity_provider: 'projects/969370446235/locations/global/workloadIdentityPools/github/providers/my-repo'
          service_account: 'github-actions-deployer@sisukas-fastapi.iam.gserviceaccount.com'

      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          name: courses-artifacts
          path: artifacts

      - name: Install gcloud CLI (storage)
        run: |
          sudo apt-get update
          sudo apt-get install -y google-cloud-cli

      - name: Upload courses.json.gz (renamed to courses.json)
        env:
          BUCKET: ${{ needs.prepare_data.outputs.gcs_bucket }}
        run: |
          gcloud storage cp artifacts/courses.json.gz gs://$BUCKET/courses.json \
            --content-type="application/json" --content-encoding="gzip"

      - name: Upload historical.json.gz (renamed to historical.json)
        if: needs.prepare_data.outputs.update_historical == 'true'
        env:
          BUCKET: ${{ needs.prepare_data.outputs.gcs_bucket }}
        run: |
          gcloud storage cp artifacts/historical.json.gz gs://$BUCKET/historical.json \
            --content-type="application/json" --content-encoding="gzip"

      - name: Upload course.json hash file (no caching)
        env:
          BUCKET: ${{ needs.prepare_data.outputs.gcs_bucket }}
        run: |
          gcloud storage cp artifacts/courses.hash.json gs://$BUCKET/courses.hash.json \
            --content-type="application/json" --cache-control="no-store, max-age=0"
      
      - name: Upload historical.json hash file (no caching)
        if: needs.prepare_data.outputs.update_historical == 'true'
        env:
          BUCKET: ${{ needs.prepare_data.outputs.gcs_bucket }}
        run: |
          gcloud storage cp artifacts/historical.hash.json gs://$BUCKET/historical.hash.json \
            --content-type="application/json" --cache-control="no-store, max-age=0"

      - name: Upload log files
        env:
          BUCKET: ${{ needs.prepare_data.outputs.gcs_bucket }}
        run: |
          gcloud storage cp artifacts/course_changes_*.json \
            gs://$BUCKET/logs/